{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TrpYNXTgKv1"
   },
   "source": [
    "Importing the 20NewsGroups dataset consisting of 11314 articles in the training dataset and 7532  articles in the test dataset accross 20 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0OgD2WhhlN5v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qHqTygNfiJX5"
   },
   "source": [
    "Next we Vectorize the articles in the Corpus.\n",
    "For this we use sci-kit learn's CountVectorizer to create a sparse matrix of the count of each word in an article\n",
    "For better results we then calculate the inverse term frequency for the words using sci-kit learn's TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "E9Nj5GYYQwos",
    "outputId": "4d7e3e12-a3d5-453b-8183-2aadb2f8b0b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "iwG-KEgXlPMB",
    "outputId": "c1920625-74d7-4884-acf8-b65fffe8a372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to vectorize the training data: 3.897 secs\n",
      "Sample sparse matrix after vectorization:\n",
      "  (0, 4605)\t0.06332603952480324\n",
      "  (0, 16574)\t0.14155752531572685\n",
      "  (0, 18299)\t0.138749083899155\n",
      "  (1, 7797)\t0.13724375024886204\n",
      "  (1, 2927)\t0.05212944077716299\n",
      "  (2, 12197)\t0.05168179280403425\n",
      "  (2, 15032)\t0.07834044496813063\n",
      "  (2, 6449)\t0.0681281384860916\n",
      "  (2, 5023)\t0.13698619641739623\n",
      "  (2, 5811)\t0.2878251559842457\n",
      "  (2, 6028)\t0.10554465088856506\n",
      "  (2, 3412)\t0.0622873125208309\n",
      "  (3, 4155)\t0.05353413616615429\n",
      "  (3, 18618)\t0.14195950717692907\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "t1=time.time()\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (\"Time take to vectorize the training data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Sample sparse matrix after vectorization:\")\n",
    "print (X_train_tfidf[0:4,0:20000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAOjSZD9iSHC"
   },
   "source": [
    "Having got the sparse matrix, we would now apply classification Algorithms on this vectorized word matrix to predic classes for data in test dataset.\n",
    "Starting with K-Nearest Neighbours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "mSh6vcpgjJMb",
    "outputId": "4dfb7a5b-0b9d-4b17-930a-811f25b28d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 3.703 secs\n",
      "Time take to Predict classes for testing data: 17.957 secs\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6591874668082847"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import neighbors\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', neighbors.KNeighborsClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy:\")\n",
    "np.mean(predicted == test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaZLIzBLl_Zp"
   },
   "source": [
    "Now we apply Support Vector Machine algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "8SeVsWJQlhOm",
    "outputId": "7355bbd3-146c-4fb0-8361-d13f6e71288f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 3.893 secs\n",
      "Time take to Predict classes for testing data: 1.937 secs\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8507700477960701"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy:\")\n",
    "np.mean(predicted == test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDhk52VRl0pD"
   },
   "source": [
    "Now we apply Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "o3hXM3H9kGIe",
    "outputId": "ca32071a-0044-4bb8-bb0e-187cd65522e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 3.469 secs\n",
      "Time take to Predict classes for testing data: 1.913 secs\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy:\")\n",
    "np.mean(predicted == test.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yqLnUNgbmBps"
   },
   "source": [
    "All this while we used Bag-Of-Words technique to vectorize the dataset.\n",
    "Here we apply ngrams technique to create the sparse matrix. Let's have an example as how n-grams is differnt from Bag-of-Words and what it actually does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "ZXp_i8uzoRu1",
    "outputId": "03602872-d63d-48ef-d039-338a02bf074c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words\n",
      "['anagh', 'anmol', 'intelligent', 'is', 'smart']\n",
      "[[2 0 1 1 0]\n",
      " [0 1 0 1 1]]\n",
      "Bi-grams\n",
      "[' a', 'ag', 'an', 'gh', 'h ', 'l ', 'mo', 'na', 'nm', 'ol']\n",
      "[[1 1 1 1 1 0 0 1 0 0]\n",
      " [1 0 1 0 0 1 1 0 1 1]]\n",
      "Tri-grams\n",
      "[' an', 'agh', 'ana', 'anm', 'gh ', 'mol', 'nag', 'nmo', 'ol ']\n",
      "[[1 1 1 0 1 0 1 0 0]\n",
      " [1 0 0 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer()\n",
    "counts = ngram_vectorizer.fit_transform(['Anagh Anagh is intelligent', 'Anmol is smart'])\n",
    "print(\"Bag-of-Words\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
    "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
    "print(\"Bi-grams\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))\n",
    "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
    "print(\"Tri-grams\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHgbmYdlrimI"
   },
   "source": [
    "What if we need to apply Bag-of-2grams, or in other words club two consecutive words in a document, then vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "lraA3HVTrvp9",
    "outputId": "3ea993f3-e2d5-43eb-c50c-888185d088fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-2grams\n",
      "['be prayagraj', 'is allahabad', 'it is', 'it will', 'today it', 'tomorrow it', 'will be']\n",
      "[[0 1 1 0 1 0 0]\n",
      " [1 0 0 1 0 1 1]]\n",
      "Time take to vectorize the training data: 22.323 secs\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "counts = ngram_vectorizer.fit_transform(['Today it is Allahabad', 'Tomorrow it will be Prayagraj'])\n",
    "print(\"Bag-of-2grams\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n",
    "t1=time.time()\n",
    "count_vect = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (\"Time take to vectorize the training data:\", round(time.time()-t1, 3), \"secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2GhI2nohoSSf"
   },
   "source": [
    "Now we will apply n-grams on our dataset and will then apply SVM classification algorithm. We will compare the accuracies for uni-gram, bi-gram and tri-gram vectorization on character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "ONMMNbOumXRY",
    "outputId": "53281e0c-d7a7-4771-ba24-ec08b73658ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR CHARACTER LEVEL n-GRAMS\n",
      "-------------------\n",
      "UNI-GRAMS\n",
      "Time take to Vectorize training data and train model: 12.643 secs\n",
      "Accuracy :  0.1488316516197557\n",
      "--------------------\n",
      "BI-GRAMS\n",
      "Time take to Vectorize training data and train model: 14.76 secs\n",
      "Accuracy :  0.6282527881040892\n",
      "--------------------\n",
      "TRI-GRAMS\n",
      "Time take to Vectorize training data and train model: 19.655 secs\n",
      "Accuracy :  0.8082846521508231\n",
      "--------------------\n",
      "4-GRAMS\n",
      "Time take to Vectorize training data and train model: 24.199 secs\n",
      "Accuracy :  0.830987785448752\n",
      "--------------------\n",
      "5-GRAMS\n",
      "Time take to Vectorize training data and train model: 25.384 secs\n",
      "Accuracy :  0.8319171534784918\n",
      "--------------------\n",
      "6-GRAMS\n",
      "Time take to Vectorize training data and train model: 24.733 secs\n",
      "Accuracy :  0.8248805098247477\n"
     ]
    }
   ],
   "source": [
    "print(\"FOR CHARACTER LEVEL n-GRAMS\")\n",
    "print(\"-------------------\")\n",
    "print(\"UNI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(1, 1))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"BI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(\"TRI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"4-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(4, 4))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"5-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"6-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(6, 6))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3P5BQlbsMI3"
   },
   "source": [
    "Now we apply N-Grams with Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "0P7GK7f5-BqZ",
    "outputId": "9d5a14c3-7788-4673-aea1-dc81a2efff8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 24.387 secs\n",
      "Time take to Predict classes for Test data: 9.613 secs\n",
      "Accuracy :  0.7015400955921403\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for Test data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHe5EOlnsSOt"
   },
   "source": [
    "Now we apply N-Grams with KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "yNFPEfbuBT13",
    "outputId": "692a47d4-cfc1-4999-9f72-8742e1bd23d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 22.5 secs\n",
      "Time take to predict classes for Test data: 48.382 secs\n",
      "Accuracy :  0.5728890069038768\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', neighbors.KNeighborsClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to predict classes for Test data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PADWu_yqtZr"
   },
   "source": [
    "Now applying Bag-of-1gram(Same as bag of words), Bag-of-2grams and Bag-of-3grams to our dataset this time grouping words together instead of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "svJR_6Anqs2m",
    "outputId": "829a60ba-7f43-4947-e2cf-36a20c9251a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD LEVEL n-GRAMS\n",
      "--------------------\n",
      "UNI-GRAMS\n",
      "Time take to Vectorize training data and train model: 4.033 secs\n",
      "Accuracy :  0.8515666489644185\n",
      "--------------------\n",
      "BI-GRAMS\n",
      "Time take to Vectorize training data and train model: 14.696 secs\n",
      "Accuracy :  0.8017790759426447\n",
      "--------------------\n",
      "TRI-GRAMS\n",
      "Time take to Vectorize training data and train model: 23.472 secs\n",
      "Accuracy :  0.7177376526818906\n"
     ]
    }
   ],
   "source": [
    "print(\"WORD LEVEL n-GRAMS\")\n",
    "print(\"--------------------\")\n",
    "print(\"UNI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(1, 1))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"BI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(2, 2))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"TRI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(3, 3))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUjUnpaKvXzk"
   },
   "source": [
    "Now we apply Word level CNN on the same dataset and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "colab_type": "code",
    "id": "ZOe9kwl0Yjcr",
    "outputId": "2b93741f-173e-4df0-cb46-04de9931930b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               25600512  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 25,873,428\n",
      "Trainable params: 25,873,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/10\n",
      "10182/10182 [==============================] - 58s 6ms/step - loss: 0.9191 - acc: 0.7626 - val_loss: 0.3024 - val_acc: 0.9214\n",
      "Epoch 2/10\n",
      "10182/10182 [==============================] - 56s 5ms/step - loss: 0.0939 - acc: 0.9819 - val_loss: 0.3159 - val_acc: 0.9117\n",
      "Epoch 3/10\n",
      "10182/10182 [==============================] - 56s 5ms/step - loss: 0.0508 - acc: 0.9927 - val_loss: 0.3368 - val_acc: 0.9125\n",
      "Epoch 4/10\n",
      "10182/10182 [==============================] - 56s 5ms/step - loss: 0.0350 - acc: 0.9958 - val_loss: 0.3232 - val_acc: 0.9214\n",
      "Epoch 5/10\n",
      "10182/10182 [==============================] - 55s 5ms/step - loss: 0.0319 - acc: 0.9966 - val_loss: 0.3023 - val_acc: 0.9240\n",
      "Epoch 6/10\n",
      "10182/10182 [==============================] - 55s 5ms/step - loss: 0.0247 - acc: 0.9975 - val_loss: 0.2998 - val_acc: 0.9223\n",
      "Epoch 7/10\n",
      "10182/10182 [==============================] - 57s 6ms/step - loss: 0.0236 - acc: 0.9978 - val_loss: 0.3590 - val_acc: 0.9196\n",
      "Epoch 8/10\n",
      "10182/10182 [==============================] - 56s 5ms/step - loss: 0.0415 - acc: 0.9960 - val_loss: 0.4615 - val_acc: 0.9055\n",
      "Epoch 9/10\n",
      "10182/10182 [==============================] - 56s 6ms/step - loss: 0.0297 - acc: 0.9972 - val_loss: 0.3382 - val_acc: 0.9161\n",
      "Epoch 10/10\n",
      "10182/10182 [==============================] - 56s 5ms/step - loss: 0.0311 - acc: 0.9971 - val_loss: 0.3571 - val_acc: 0.9134\n",
      "7532/7532 [==============================] - 10s 1ms/step\n",
      "Time take to Predict classes for testing data: 10.04 secs\n",
      "Test accuracy: 0.816117892351419\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "np.random.seed(1237)\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "label_index = train.target\n",
    "label_names = train.target_names\n",
    "labelled_files = train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],train.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "train_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "label_index = test.target\n",
    "label_names = test.target_names\n",
    "labelled_files = test.filenames\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],test.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "test_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "train_posts = train_data['news'][:]\n",
    "train_tags = train_data['category'][:]\n",
    "train_files_names = train_data['filename'][:]\n",
    " \n",
    "test_posts = test_data['news'][:]\n",
    "test_tags = test_data['category'][:]\n",
    "test_files_names = test_data['filename'][:]\n",
    "\n",
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 50000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    " \n",
    "\n",
    "\t\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.333))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "t1=time.time()\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyQhdnforgk8"
   },
   "source": [
    "Now we apply CNN on data pre processed with N-Grams, taking value of N as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1198
    },
    "colab_type": "code",
    "id": "4dbwgJwKGkNH",
    "outputId": "e5120489-5006-427f-f89a-996b40852f2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               15360512  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 15,633,428\n",
      "Trainable params: 15,633,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/20\n",
      "10182/10182 [==============================] - 35s 3ms/step - loss: 1.8682 - acc: 0.5438 - val_loss: 0.6674 - val_acc: 0.8295\n",
      "Epoch 2/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.2434 - acc: 0.9415 - val_loss: 0.4812 - val_acc: 0.8613\n",
      "Epoch 3/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0367 - acc: 0.9949 - val_loss: 0.4669 - val_acc: 0.8684\n",
      "Epoch 4/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0152 - acc: 0.9979 - val_loss: 0.4883 - val_acc: 0.8631\n",
      "Epoch 5/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0095 - acc: 0.9983 - val_loss: 0.4877 - val_acc: 0.8693\n",
      "Epoch 6/20\n",
      "10182/10182 [==============================] - 34s 3ms/step - loss: 0.0056 - acc: 0.9989 - val_loss: 0.4942 - val_acc: 0.8701\n",
      "Epoch 7/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0062 - acc: 0.9990 - val_loss: 0.5158 - val_acc: 0.8657\n",
      "Epoch 8/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0050 - acc: 0.9989 - val_loss: 0.5080 - val_acc: 0.8737\n",
      "Epoch 9/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0057 - acc: 0.9989 - val_loss: 0.5211 - val_acc: 0.8684\n",
      "Epoch 10/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.5190 - val_acc: 0.8719\n",
      "Epoch 11/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.5190 - val_acc: 0.8710\n",
      "Epoch 12/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0052 - acc: 0.9990 - val_loss: 0.5297 - val_acc: 0.8701\n",
      "Epoch 13/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.5550 - val_acc: 0.8631\n",
      "Epoch 14/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.5368 - val_acc: 0.8719\n",
      "Epoch 15/20\n",
      "10182/10182 [==============================] - 34s 3ms/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.5319 - val_acc: 0.8737\n",
      "Epoch 16/20\n",
      "10182/10182 [==============================] - 33s 3ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 0.5395 - val_acc: 0.8693\n",
      "Epoch 17/20\n",
      "10182/10182 [==============================] - 34s 3ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 0.5699 - val_acc: 0.8631\n",
      "Epoch 18/20\n",
      "10182/10182 [==============================] - 34s 3ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.5589 - val_acc: 0.8710\n",
      "Epoch 19/20\n",
      "10182/10182 [==============================] - 34s 3ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.5727 - val_acc: 0.8666\n",
      "Epoch 20/20\n",
      "10182/10182 [==============================] - 34s 3ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.5580 - val_acc: 0.8675\n",
      "7532/7532 [==============================] - 6s 801us/step\n",
      "Time take to Predict classes for testing data: 6.036 secs\n",
      "Test accuracy: 0.06439192765216782\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "np.random.seed(1237)\n",
    "import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(2, 2), max_features=30000)\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(2, 2), max_features=30000)\n",
    "X_train_counts = count_vect.fit_transform(test.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "label_index = train.target\n",
    "label_names = train.target_names\n",
    "labelled_files = train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],train.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "train_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "label_index = test.target\n",
    "label_names = test.target_names\n",
    "labelled_files = test.filenames\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],test.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "test_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "train_posts = train_data['news'][:]\n",
    "train_tags = train_data['category'][:]\n",
    "train_files_names = train_data['filename'][:]\n",
    " \n",
    "test_posts = test_data['news'][:]\n",
    "test_tags = test_data['category'][:]\n",
    "test_files_names = test_data['filename'][:]\n",
    "\n",
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 50000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    " \n",
    "\n",
    "\t\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(30000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.333))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(X_train_tfidf.todense(), y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "t1=time.time()\n",
    "score = model.evaluate(X_test_tfidf, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Text_Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
